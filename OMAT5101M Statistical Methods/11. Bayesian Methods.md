# Conditional Distributions
In the frequentist approach, which is what we have been doing so far, we consider a distribution with a deterministic parameter, however in the bayesian approach, the parameter itself is also a random variable, so now we have 2 sources of randomness, the parameter and the distribution dependent on the parameter. This leads us to consider conditional distributions. Essentially it is the same as conditional probability but with the probability densities:
$$f_X(x|Y=y)=\frac{f_{X,Y}(x,y)}{f_Y(y)}$$
Similarly, we can define chain rules for densities
$$f_{X,Y}(x,y)=f_Y(y)f_X(x|Y=y)=f_X(x)f_Y(y|X=x)$$
And so we have the Bayes Formula:
$$f_X(x|Y=y)=\frac{f_X(x)f_Y(y|X=x)}{f_Y(y)}$$
After defining conditional densities, it is natural to look at a corresponding conditional expectation. Clearly, this should just be the expectation with respect to the conditional density.
$$E[X|Y=y]=\int xf_X(x|Y=y)dx$$
Or alternatively without fixing the value of $Y$, so the expectation itself is also a random variable:
$$E[X|Y]=\int xf_X(x|Y)dx$$
Some useful properties to note:
- Conditional expectation of $X$ given $Y$ is a function of $Y$:
$$E[X|Y]=g(Y)$$
- Total Expectation:
$$E[X]=E[E[X|Y]]$$
- If $X$ and $Y$ are independent:
$$E[X|Y]=E[X]$$
- If $Z=h(Y)$, then:
$$E[ZX|Y]=Z\cdot E[X|Y]$$
# Bayesian Approach
With the tools we have developed so far, we can use the Bayes Formula to update our beliefs based on available data. Essentially we obtain a posterior probability based on our prior and the data.
$$Prob(model|data)=\frac{Prob(data|model)Prob(model)}{Prob(data)}$$
# MSE and Bayes Estimator
The MSE was defined as 
$$MSE(\hat{\theta})=E[(\hat{\theta}-\theta)^2]$$
In the Bayesian approach we have
$$MSE(\hat{\theta})=E[E_\theta[(\hat{\theta}-\theta)^2]]=\int_\Theta E_\theta[(\hat{\theta}-\theta)^2]q(\theta)d\theta$$
Where $q(\theta)$ is the prior.
$$E_\theta[(\hat{\theta}-\theta)^2]=Var(\hat{\theta})+[E[\hat{\theta}]-\theta]^2$$
Having chosen a prior, it is natural to look for an optimal estimator such that its MSE is as small as possible. Such an estimator is called a Bayes Estimator. In fact the Bayes estimator is given by the posterior mean:
$$\hat{\theta}_B = E[\theta|\mathbf{X}]=\int_\Theta \theta q(\theta|\mathbf{X})d\theta$$
# Conjugate Priors
Idea is for the prior and the posterior to belong to the same distribution family, so we consider the following 3.
## [[Beta Distribution]] prior in Bernoulli
Let $\mathbf{X}$ be a random sample from the Bernoulli distribution, where $T=X_1+...+X_n$.
$$q(\theta)\sim Beta(\alpha,\beta)\Rightarrow q(\theta|\mathbf{X})\sim Beta(T+\alpha,n-T+\beta)$$
And so the Bayes Estimator is:
$$\hat{\theta}_B=\frac{T+\alpha}{n+\alpha+\beta}$$
## [[Gamma Distribution]] Prior in Poisson
Let $\mathbf{X}$ be a random sample from the Poisson distribution, where $T=X_1+...+X_n$.
$$q(\theta)\sim Gamma(\alpha,\lambda)\Rightarrow q(\theta|\mathbf{X})\sim Gamma(T+\alpha,n+\lambda)$$
So the Bayes Estimator is:
$$\hat{\theta}_B=\frac{T+\alpha}{n+\lambda}$$
## Normal Prior in Normal
Let $\mathbf{X}$ be a random sample from the normal distribution $N(\theta,\sigma^2)$, where $T=X_1+...+X_n$, with unknown mean and known variance.
$$q(\theta)\sim N(a,b^2)\Rightarrow q(\theta|\mathbf{X})\sim N\left(\frac{T/\sigma^2+a/b^2}{n/\sigma^2+1/b^2},\frac{1}{n/\sigma^2+1/b^2}\right)$$
So the Bayes Estimator is:
$$\hat{\theta}_B=\frac{T/\sigma^2+a/b^2}{n/\sigma^2+1/b^2}$$
# Markov Chain Monte Carlo Simulation
If say we wish to use a non conjugate prior, we could calculate the posterior distribution, but sometimes this may be very complicated or simply not possible. However, instead of trying to write down a density function for the posterior, we can apply an MCMC method called the Metropolis (or Metropolis-Hastings) algorithm. Its general design is as follows.

Consider data $X_1, \ldots, X_n$ with the likelihood function $L\left(\theta \mid X_1, \ldots, X_n\right)$ for the unknown parameter $\theta$, and let $q(\theta)$ be a chosen prior density for $\theta$. Our aim is to generate a random sample $\theta_1, \theta_2, \ldots$ with the posterior density
$$
q\left(\theta \mid X_1, \ldots, X_n\right) \propto L\left(\theta \mid X_1, \ldots, X_n\right) \times q(\theta) .
$$
The Metropolis algorithms is implemented through the following steps:
1. Set $t=0$ and choose an initial value $\theta_0$.
2. Sample a random number $\delta$ from a normal distribution $\mathrm{N}\left(0, c^2\right)$.
3. Set $\tilde{\theta}=\theta_t+\delta$ (proposal).
4. Calculate the acceptance ratio
$$
a=\frac{q\left(\tilde{\theta} \mid X_1, \ldots, X_n\right)}{q\left(\theta_t \mid X_1, \ldots, X_n\right)}=\frac{L\left(\tilde{\theta} \mid X_1, \ldots, X_n\right) \times q(\tilde{\theta})}{L\left(\theta_t \mid X_1, \ldots, X_n\right) \times q\left(\theta_t\right)} .
$$
5. Sample a random number $u$ from the uniform distribution $\operatorname{Unif}(0,1)$.
6. If $u<a$ then accept the proposal and set $\theta_{t+1}=\tilde{\theta}$. Otherwise, reject the proposal and copy the previous value forward by setting $\theta_{t+1}=\theta_t$.
7. Replace $t$ with $t+1$ and repeat steps 2 to 7.

Example:
```R
set.seed(58) # this ensures random results are reproducible 
nsim <- 100000; mu0 <- 36; sigma0 <- 2; sigma <- 3; xbar <- 34 sigma10 <- sigma/sqrt(10); sigmasq10 <- sigma10^2 musim <- mu0 + sigma0*rnorm(nsim) # 100000 simulated values of mu from prior 
w <- exp(-.5*(musim - xbar)^2/sigmasq10) # 100000 weights at simulated values of mu 
mu.post <- sum(w * musim) / sum(w) 
sigmasq.post <- sum(w * (musim - mu.post)^2) / sum(w) 
sigma.post <- sqrt(sigmasq.post) 
cat("posterior mean, variance and sd for mu:", "\n", mu.post, sigmasq.post, sigma.post, "\n") ## posterior mean, variance and sd for mu: ## 34.36601 0.7369863 0.8584791
```
# Hypothesis Testing
We start with simple hypothesis
$$H_0:\theta=\theta_0,\qquad H_1:\theta=\theta_1$$
Note that these hypothesis have equal status and there is no null hypothesis/alternative. Under the Bayesian approach, it is assumed that the hypotheses $H_0$ and $H_1$ are true with prior probabilities $q_0=q\left(\theta_0\right)$ and $q_1=q\left(\theta_1\right)$, respectively, with $q_0+q_1=1$. The ratio
$$
Q=\frac{q\left(\theta_0\right)}{q\left(\theta_1\right)}
$$
is called the prior odds. Given a random sample $\boldsymbol{X}$, consider the posterior odds
$$
Q^*=\frac{q\left(\theta_0 \mid \boldsymbol{X}\right)}{q\left(\theta_1 \mid \boldsymbol{X}\right)}
$$
and also introduce the Bayes factor
$$
B=\frac{L\left(\theta_0 \mid \boldsymbol{X}\right)}{L\left(\theta_1 \mid \boldsymbol{X}\right)}
$$
Using the Bayes formula for the posterior density
$$
q(\theta \mid \boldsymbol{X}) \propto L(\theta \mid \boldsymbol{X}) \cdot q(\theta),
$$
where the proportionality symbol $\propto$ as usual indicates omission of factors depending only on the data $\boldsymbol{X}$, the posterior odds $Q^*$ can be expressed as
$$
Q^*=\frac{q\left(\theta_0 \mid \boldsymbol{X}\right)}{q\left(\theta_1 \mid \boldsymbol{X}\right)}=\frac{L\left(\theta_0 \mid \boldsymbol{X}\right)}{L\left(\theta_1 \mid \boldsymbol{X}\right)} \cdot \frac{q\left(\theta_0\right)}{q\left(\theta_1\right)} .
$$
Combining this with the definitions of the prior odds $Q$ and the Bayes factor $B$, we obtain the important relation
$$
Q^*=B \cdot Q
$$
Then if $Q^*<1$, accept $H_1$, otherwise accept $H_0$. 
# Credible Intervals
Bayesian version of a confidence interval where the interval has a certain percentage chance to be within the posterior density interval. Given $\alpha \in(0,1)$, a set $S_{\boldsymbol{x}} \subset \Theta\left(\boldsymbol{x} \in \mathbb{R}^n\right)$ is said to be a credible set for $\theta$ with credible level $1-\alpha$ if
$$
\mathrm{P}\left(\theta \in S_{\boldsymbol{x}} \mid \boldsymbol{X}=\boldsymbol{x}\right)=1-\alpha
$$
or, in integral form,
$$
\int_{S_{\boldsymbol{x}}} q(\theta \mid \boldsymbol{x}) \mathrm{d} \theta=1-\alpha .
$$
Thus, the posterior probability that $\theta$ belongs to the set $S_{\boldsymbol{x}}$ (given the sample value $\boldsymbol{X}=\boldsymbol{x}$ ), equals $1-\alpha$.

For a given $\alpha \in(0,1)$, generally there are many regions of credible level $1-\alpha$, so some criterion is needed to choose between them. Since $S_{\boldsymbol{x}}$ has the meaning of a "plausible" region for $\theta$, it is natural to require that
$$
q(\theta \mid \boldsymbol{x}) \geq q\left(\theta^{\prime} \mid \boldsymbol{x}\right) \text { for all } \theta \in S_{\boldsymbol{x}} \text { and } \theta^{\prime} \notin S_{\boldsymbol{x}}
$$
Then, any value $\theta$ included in $S_{\boldsymbol{x}}$ is at least as probable as any excluded value. A $(1-\alpha)$-credible set $S_x$ satisfying the previous condition is called the highest posterior density (HPD) credible region. This credible interval also has the nice property of having the smallest length.
