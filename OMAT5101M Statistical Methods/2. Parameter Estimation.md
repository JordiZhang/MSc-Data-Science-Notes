Estimating parameters of a distribution based on empirical data that samples from it. 
# Likelihood
The Likelihood $L$ of a parameter $\theta$, given the observed sample $\vec{X}$, is defined as 
$$L(\theta)=L(\theta|\vec{X})=f(\vec{X}|\theta)=\prod^{n}_{i=1}f(X_i|\theta)$$
The log likelihood $l$ of a parameter \theta, given the observed sample $\vec{X}$, is defined as
$$l(\theta)=l(\theta|\vec{X})=\ln L(\vec{X}|\theta)=\sum^{n}_{i=1}\ln f(X_i|\theta)$$
# Estimators and Bias
An estimator is a function of the sample only, thus it has no dependence on the real value of the parameter $\theta$, that is to say it is any variable computable from the observed data. 

The Bias of an estimator is $\hat{\theta}$ is defined as the mean deviation of $\hat{\theta}$ from the true value of $\theta$
$$b(\theta)=E[\hat{\theta}]-\theta$$
Thus an estimator is unbiased if $b(\theta)=0$, or said in another way $E[\hat{\theta}]=\theta$. Some examples of unbiased estimators are: the sample mean and variance.

We can characterize the random error of the estimation via its Mean Square Error (MSE)
$$MSE_\theta (\hat{\theta})=E[(\hat{\theta}-\theta)^2]=Var_\theta(\hat{\theta})+(b(\theta))^2$$
# Consistency of Estimators
An important issue is the performance of estimators. We would like to ensure that the error in the estimator gets smaller as we increase the sample size. 

An estimator is said to be consistent for parameter $\theta$ if it is asymptotically close to $\theta$ for large sample sizes. More precisely, for any tolerance $\varepsilon>0$, the following should be true
$$\lim_{n\rightarrow \infty}P_\theta (|\hat{\theta}-\theta|>\varepsilon)=0$$
Or in other words that the estimator converges to the parameter $\theta$ as $n\rightarrow \infty$. One easy way of testing the consistency of an estimator is via its MSE. If the MSE of an estimator converges to $0$ then the estimator is consistent.
# Fisher's Information
Fisher's Information is a way of measuring the amount of information that an observable random variable contains about the parameter $\theta$. It is defined as the expectation value of the derivative of the log likelihood squared:
$$I_n(\theta)=E_\theta[(l_\theta')^2]$$
Some useful identities to know relating to Fisher's Information:
- $E_\theta[l_\theta']=0$ (Log likelihood identity 1)
- For an unbiased estimator, $E_\theta[\hat{\theta}l_\theta']=1$ (Log likelihood identity 2)
- $I_n(\theta)=Var_\theta(l_\theta')$
- Fisher's Information is additive and grows with sample size
$$I_n(\theta)=nI_1(\theta)$$
- An Alternate definition of Fisher's Information is done using the second derivative
$$I_n(\theta)=-E_\theta[l_{\theta\theta}'']$$

For some standard distributions:
- $X\sim Pois(\theta)$: 
$$I_n(\theta)=\frac{n}{\theta}$$
- $X\sim Geom(\theta)$: 
$$I_n(\theta)=\frac{n}{\theta^2(1-\theta)}$$
- $X\sim Exp(\theta^{-1})$: 
$$I_n(\theta)=\frac{n}{\theta^2}$$
- $X\sim N(\theta,\sigma^2)$: 
$$I_n(\theta)=\frac{n}{\sigma^2}=\text{const}$$
- $X\sim N(\mu,\theta)$: 
$$I_n(\theta)=\frac{n}{2\theta^2}$$

# Efficient Estimators
The Cramer-Rau Inequality gives us a lower bound of the MSE. In particular for a biased estimator we have:
$$MSE_\theta(\hat{\theta})\geq \frac{(1+b'(\theta))^2}{I_n(\theta)}+b(\theta)^2$$
For an unbiased one:
$$MSE_\theta(\hat{\theta})\geq \frac{1}{I_n(\theta)}$$
Thus we define an efficient estimator as one which achieves said lower such that 
$$MSE_\theta(\hat{\theta})=\frac{1}{I_n(\theta)}$$
There also exist super efficient estimators which are estimators whose MSE is below the Cramer-Rau lower bound. However, this contradiction is resolved by noting that in these cases, the log-likelihood identities are not valid, and thus their Cramer-Rau Inequalities aren't either.
# Maximum Likelihood Estimation
An estimator is called the Maximum Likelihood Estimator if it maximizes the Likelihood Function, where the maximization is taken over the entire parameter space $\Theta =\{\theta\}$ with a given sample. As with any other maximization problem, compute the derivative of the likelihood and set to 0. It is however, more convenient to maximize the log likelihood instead. One must check that this is indeed the maximum, a sufficient condition is
$$l''_{\theta\theta}|_{\theta=\hat{\theta}}<0$$

For some Standard Distributions:
-  $X\sim Pois(\theta)$: 
$$\hat{\theta}=\bar{X}$$
- $X\sim Geom(\theta)$: 
$$\hat{\theta}=\frac{1}{\bar{X}}$$
- $X\sim Exp(\theta^{-1})$: 
$$\hat{\theta}=\bar{X}$$
- $X\sim N(\theta,\sigma^2)$: 
$$\hat{\theta}=\bar{X}$$
- $X\sim N(\mu,\theta)$: 
$$\hat{\theta}=\hat{\sigma}^2=\frac{1}{n}\sum^n_{i=1}(X_i-\mu)^2$$
- $X\sim Unif(0, \theta)$:
$$\hat{\theta}=U_{(n)}$$
	From order statistics, i.e. the maximum
MLE is consistent.

Asymptotic Normality of MLE
As $n\rightarrow \infty$, the MLE $\hat{\theta}$ is asymptotically normal with mean $\theta$ and variance $1/I_n(\theta)$.
$$\hat{\theta}\approx N(\theta,I_n^{-1}(\theta))$$
Or written in another way
$$\sqrt{n}(\hat{\theta}-\theta)\rightarrow N(0,I^{-1}(\theta))$$
Then according to this property of the MLE, $\hat{\theta}$ is asymptotically biased as the mean is exactly $\theta$ and is also asymptotically efficient as the variance is exactly equal to the Cramer-Rau Bound.
# Method of Moments
The idea is that the sample mean should be close to the actual mean, justified by the law of large numbers. 
$$\bar{X}\approx \mu(\theta)$$
So inverting it we have the following estimator
$$\hat{\theta}=\mu^{-1}(\bar{X})$$
We can extend this idea by taking successively higher moments. For example for a normal distribution $N(\mu, \theta)$, the first moment is $\mu_1=E[X]=\mu$, which has no $\theta$ dependence. Instead we can take the 2nd moment $\mu_2=E[X^2]=\theta + \mu^2$, so 
$$\hat{\theta} = \overline{X^2}-\mu^2$$
Which after some rearrangement we find to be the variance $\hat{\sigma}^2$.

The method of moments estimator is consistent, and it is also asymptotically normal with mean $\theta_0$ and variance $((\mu^{-1})'_{\mu(\theta_0)})^2\sigma^2(\theta_0)/n$. Despite having a relatively simple computation and good asymptotic properties, MoM estimators are in general not efficient. MoM are hence used to provide reasonably quick solutions and are frequently used as a good starting point in iterative numerical schemes needed for MLE computation.
# Standard Errors
When doing parameter estimation, it is important to also be able to assess the accuracy of our estimates in terms of deviations from its true value. In theory, we do this using the MSE, which is useful to compare different estimators. However, it doesn't tell us much about the error of a concrete numerical value $\hat{\theta}$ calculated from the sample $X$. To be able to do that, we need to estimate the MSE rather, since normally we do not know the real value.

We define the standard error as such
$$se(\hat{\theta})=\sqrt{\hat{\eta}}\approx \sqrt{MSE_\theta(\hat{\theta})}$$
Or 
$$se(\hat{\theta})=\sqrt{\hat{\sigma}^2}=\hat{\sigma}$$
Say we have trouble computing the exact variance of the sample, to overcome this we could leverage the fact that our estimator is asymptotically normal with some defined variance given by our methodology, MSE -> $1/I_n(\theta)$. Thus 
$$se(\hat{\theta})=\frac{1}{\sqrt{I_n(\hat{\theta})}}$$
If our estimator is not asymptotically normal but its distribution admits a different approximation, this can also be used to evaluate the standard error as is the case with the Uniform distribution.

An alternative approach to numerical evaluation of the standard error is through Monte Carlo Simulations, also called the Bootstrap method.
- For a given dataset, calculate the numerical value of the estimate $\hat{\theta}$
- Generate a large number $N$ of sample size $n$ using $\hat{\theta}$ as the model parameter $\theta$
- For each bootstrap sample, calculate the corresponding value of the estimator $\hat{\theta}_i$
- Evaluate the standard error $se(\hat{\theta})$ by calculating the standard deviation of the bootstrap set $(\hat{\theta}_i)$
$$se(\hat{\theta})=sd(\hat{\theta}_1,...,\hat{\theta}_N)$$