We won't go over the basics such as what is a null hypothesis, one and 2 sided tests. Only notes on things that we may need a refresher on or new things we have not covered before. 

The traditional approach to hypothesis testing is to control in the first instance the probability of Type I error and subject to that constraint, try to minimize the probability of Type II error. In particular we look for tests such that
$$\max_{\theta \in \Theta_0}P_\theta(\vec{X}\in C)=\alpha$$
# Type Errors
- Type I error occurs when we reject the null hypothesis when it is in fact true
- Type II error occurs when we do not reject accept the null hypothesis when it is in fact false
- Recall that the Null Hypothesis is the hypothesis we formulate to perform the test, in the case of a coinflip, it would be that p=0.5 
# Power Function
For a test of size (significance) $\alpha$ with the corresponding critical region $C$, the power function of the test is defined by
$$\eta(\theta)=P_\theta(\vec{X}\in C),\qquad \theta \in \Theta$$
Which can also be represented as an integral of the Likelihood.
$$\eta(\theta)=\int_CL(\theta|\vec{x})dx$$
When $\Theta=\Theta_0$, the power function represents the probability of Type I error, thus from our approach to hypothesis testing and the size constraint we have
$$\max_{\theta \in \Theta_0}\eta(\theta)=\alpha$$
On the other hand if $\Theta=\Theta_1$, we have the following
$$\beta(\theta)=P_\theta(\vec{X}\notin C) = 1-P_\theta(\vec{X}\in C)$$
$$\eta(\theta)=1-\beta(\theta),\qquad \theta\in\Theta_1$$
So minimising the probability of Type II error is equivalent to maximising the the power function.
# Unbiased Tests
A desirable property of a statistical test is that the probability of Type I error is less than the probability of correctly rejecting the null hypothesis. Such a test is called an unbiased test.
$$P(\text{reject }H_0|H_0\text{ true})\leq P(\text{reject }H_0|H_0\text{ false})$$
In terms of the power function we have
$$\max_{\theta \in \Theta_0}\eta(\theta)\leq \max_{\theta\in\Theta_1}\eta(\theta)$$
# Consistency of Tests
Since the probability of Type I error is kept low by fixing the significance level, the focus is on the behaviour of the probability of Type II error. For a good test, it should tend to zero, this property is called the consistency of the test.
$$\beta(\theta)\rightarrow0\qquad(n\rightarrow \infty)$$
So in terms of the power function
$$\lim_{n\rightarrow\infty}\eta(\theta)=1\qquad (\theta\in\Theta_1)$$
# Usual Workflow
- In this example null hypothesis $H_0:\mu\leq\mu_0$ and alternative $H_1:\mu>\mu_0$
- First find the critical value for a given $\alpha$, 
$$c=\mu_0+\frac{z(\alpha)\sigma}{\sqrt{n}}$$
- Construct $\beta$ from the critical value in terms of $\Phi$ 
$$\beta(\mu)=P_\mu(\bar{X}\leq c)=P\left(Z\leq\frac{\sqrt{n}(c-\mu)}{\sigma}\right)=\Phi\left(\frac{\sqrt{n}(c-\mu)}{\sigma}\right)$$
$$\beta(\mu)=\Phi\left(z(\alpha)-\frac{\sqrt{n}(\mu-\mu_0)}{\sigma}\right)$$
- Then finally we construct the power function
$$\eta(\mu)=1-\beta(\mu)$$
- From here we can determine whether a test is biased, and whether it is consistent or not
- As a reminder, consistency:
$$\beta(\mu)=\Phi\left(z(\alpha)-\frac{\sqrt{n}(\mu-\mu_0)}{\sigma}\right)\rightarrow\Phi(-\infty)=0\qquad(n\rightarrow\infty)$$
- Unbiased, satisfies the inequality we discussed earlier:
$$\max_{\mu\leq\mu_0}\eta(\mu)=\alpha= \max_{\mu>\mu_0}\eta(\mu)$$
# Likelihood Ratio
We define the Likelihood ratio as such
$$\Lambda=\frac{L(H_0|\vec{X})}{L(H_1|\vec{X})}$$
- If $\Lambda>c$, do not reject $H_0$
- If $\Lambda<c$, reject $H_0$
By the Neyman-Pearson Lemma, the likelihood ratio test is the most powerful test for a null hypothesis $H_0:\theta=\theta_0$ against alternative $H_1:\theta=\theta_1$. This is why the likelihood ratio test is so powerful. In fact it can also be applied to two tailed tests with some caveats. When applied to a single tailed test, we get Uniformly Most Powerful tests, but when applied to two tailed ones, we don't. We won't cover the actual caveats, but most distributions we work with do satisfy them (think its monotone and something else) we can apply the lemma.