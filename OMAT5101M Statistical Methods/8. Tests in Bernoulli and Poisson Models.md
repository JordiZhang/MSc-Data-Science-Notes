Hypothesis testing under these 2 models are much more straightforward. We simply calculate the threshold value of occurrences for a given significance level and use that as critical region.
# Bernoulli/Binomial Model
As a typical example consider a manufacturer's items which can be classified as either acceptable or defective, then each item produced will independently be defective with some probability $p$. Suppose we have the following hypothesis:
$$H_0:p\leq p_0,\qquad H_1:p>p_0$$
Clearly to choose whether to reject $H_0$ or not, should be based on the number of defective items $T$, if we observe too many defective items, we reject $H_0$. We know $T\sim Bin(n,p)$, so the probability of exceedance is given by, for some threshold $k$:
$$P_p(T\geq k)=\sum^n_{i=k}\binom{n}{i}p^i(1-p)^{n-i}$$
Intuitively, this probability is an increasing function of $p$, higher probability of defectiveness, should mean higher probability of seeing more defective items. This is indeed true and this probability is monotonic, we won't prove it however. Now we can construct the binomial test. When $H_0$ is true, we have
$$P_p(T\geq k)\leq P_{p_0}(T\geq k)$$
Hence, our test will reject $H_0$ at a significance level $\alpha$ when $T\geq k^*$, where $k^*$ is the smallest value of $k$ for which 
$$
k^*=\min \left\{k: \sum_{i=k}^n\binom{n}{i} p_0^i\left(1-p_0\right)^{n-i} \leq \alpha\right\}
$$
Alternatively, we can first determine the observed value of the test statistic $T_{o b s}=t$ and then compute the $p$-value:
$$
p \text {-value }=\mathrm{P}_{p_0}(T \geq t)=\sum_{i=t}^n\binom{n}{i} p_0^i\left(1-p_0\right)^{n-i}
$$
If the test is two tailed, then simply half the significance level and we can proceed as normal with exceedance.
# Poisson Model
Suppose we have a random sample $\boldsymbol{X}=\left(X_1, \ldots, X_n\right)$ from a Poisson distribution $Pois( \lambda )$ with mean $\mathrm{E}_\lambda(X)=\lambda$. Consider a test of the hypotheses
$$
H_0: \lambda=\lambda_0, \qquad H_1: \lambda>\lambda_0 
$$
It is reasonable to use the sum $T=X_1+\cdots+X_n$ as a test statistic, e.g. recalling that the sample mean $\bar{X}=T / n$ is the maximum likelihood estimator, with $\mathrm{E}_\lambda(T / n)=\lambda$. Indeed, it is intuitively clear that large values of $T$ would discredit the null hypothesis $H_0$. Thus, the test will be of the form
$$
T \geq k^* \Rightarrow \text { reject } H_0,
$$
where the critical value $k^*$ is determined by the desired significance level $\alpha$ :
$$
\mathrm{P}_{\lambda_0}\left(T \geq k^*\right) \leq \alpha .
$$
Note that under $H_0: \lambda=\lambda_0$, the sum $T=X_1+\cdots+X_n$ has a Poisson distribution with parameter $n \lambda_0$. Hence,
$$
\mathrm{P}_{\lambda_0}\left(T \geq k^*\right)=\sum^n_{i =k^*} \frac{\left(n \lambda_0\right)^i \mathrm{e}^{-n \lambda_0}}{i!} \leq \alpha,
$$
and the suitable critical value $k^*$ is determined from the condition:
$$
k^*=\min \left\{k: \sum_{i=k}^{\infty} \frac{\left(n \lambda_0\right)^i \mathrm{e}^{-n \lambda_0}}{i!} \leq \alpha\right\}
$$
Alternatively, given the observed value $T_{o b s}=t$, we can compute the $p$-value:
$$
p \text {-value }=\mathrm{P}_{\lambda_0}(T \geq t)=\sum_{i=t}^{\infty} \frac{\left(n \lambda_0\right)^i \mathrm{e}^{-n \lambda_0}}{i!}
$$
The same test can be applied if $H_0:\lambda\leq \lambda_0$. This is due to a monotonicity lemma stating that for any $k>0$, the probability $P_\lambda(T\geq k)$ is an increasing function of $\lambda$.
# Approximate Normal Tests
## Bernoulli/Binomial Model
When the sample size $n$ is large, we can approximate the binomial distribution as a normal one. 
$$T\sim Bin(n,p) \approx N(np, np(1-p))$$
$$\frac{T-np}{\sqrt{np(1-p)}}\sim N(0,1)$$
So we can use the normal approximation to do hypothesis testing as is usual.
## Poisson Model
Similarly we have
$$T\sim Pois(n\lambda)\approx N(n\lambda,n\lambda)$$
$$\frac{T-n\lambda}{\sqrt{n\lambda}}\sim N(0,1)$$
Then use the normal approximation to do hypothesis testing as is usual.
