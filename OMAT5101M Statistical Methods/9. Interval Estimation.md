A pivotal quantity is a function $g(\theta,\mathbf{X})$ such that:
- Its distribution is known and independent of $\theta$ 
- It is monotone in $\theta$
Then for a given $\alpha$ we can choose bounds such that 
$$P_\theta(g_1\leq g(\theta,\mathbf{X}) \leq g_2)=1-\alpha$$
Because the underlying distribution is $\theta$-free, the bounds also do not depend on $\theta$. Hence, due to the monotonicity of the pivotal quantity, the inequalities can be solved for $\theta$ yielding the appropriate bounds 
$$P_\theta(\theta_1\leq \theta \leq \theta_2)=1-\alpha$$
# One Sample Intervals
## Variance: Known Mean
To construct a confidence interval for the variance $\sigma^2$, start with the estimator $\hat{\sigma}^2=\frac{1}{n} \sum_{i=1}^n\left(X_i-\mu\right)^2$ and recall that
$$
g\left(\sigma^2 ; \boldsymbol{X}\right)=\frac{n \hat{\sigma}^2}{\sigma^2} \sim \chi_n^2
$$
Using this $g\left(\sigma^2 ; \boldsymbol{X}\right)$ as a pivotal quantity, a two-sided $(1-\alpha)$-confidence interval is then given by
$$
\frac{n \hat{\sigma}^2}{\chi_n^2(\alpha / 2)} \leq \sigma^2 \leq \frac{n \hat{\sigma}^2}{\chi_n^2(1-\alpha / 2)}
$$
where $\chi_n^2(\gamma)$ is the upper $\gamma$-quantile of the $\chi_n^2$-distribution, i.e. if $Y \sim \chi_n^2$ then $\mathrm{P}\left(Y>\chi_n^2(\gamma)\right)=\gamma$.
One-sided $(1-\alpha)$-confidence bounds are straightforward:
$$
\sigma^2 \geq \frac{n \hat{\sigma}^2}{\chi_n^2(\alpha)} \qquad \sigma^2 \leq \frac{n \hat{\sigma}^2}{\chi_n^2(1-\alpha)}
$$

## Variance: Unknown Mean
To construct the confidence interval for the variance $\sigma^2$ when the mean $\mu$ is unknown, consider the estimator $S^2=\frac{1}{n-1} \sum_{i=1}^n\left(X_i-\bar{X}\right)^2$ and recall that
$$
g\left(\sigma^2 ; \boldsymbol{X}\right)=\frac{(n-1) S^2}{\sigma^2} \sim \chi_{n-1}^2
$$
which can be used as a pivotal quantity. Modifying the calculations above to the case of chi-squared distribution with $n-1$ degrees of freedom, we obtain the confidence bounds:
$$
\frac{(n-1) S^2}{\chi_{n-1}^2(\alpha / 2)} \leq \sigma^2 \leq \frac{(n-1) S^2}{\chi_{n-1}^2(1-\alpha / 2)}
$$
Similarly, one-sided $(1-\alpha)$-confidence bounds:
$$
\sigma^2 \geq \frac{(n-1) S^2}{\chi_{n-1}^2(\alpha)} \qquad \sigma^2 \leq \frac{(n-1) S^2}{\chi_{n-1}^2(1-\alpha)}
$$
## Mean: Unknown Variance
With known variance, it is just the typical interval we did in A levels.
To construct the confidence interval for the mean $\mu$, consider the estimators $\bar{X}$ and $S^2$ and recall that the following statistic has a $t$-distribution with $n-1$ degrees of freedom:
$$
g(\mu ; \boldsymbol{X})=\frac{\sqrt{n}(\bar{X}-\mu)}{S} \sim t_{n-1} .
$$
Utilising this as a pivotal quantity readily leads to the confidence bounds:
$$
\bar{X}-\frac{t_{n-1}(\alpha / 2) S}{\sqrt{n}} \leq \mu \leq \bar{X}+\frac{t_{n-1}(\alpha / 2) S}{\sqrt{n}}
$$
One-sided bounds are similar:
$$
\mu \geq \bar{X}-\frac{t_{n-1}(\alpha) S}{\sqrt{n}}
\qquad
\mu \leq \bar{X}+\frac{t_{n-1}(\alpha) S}{\sqrt{n}}
$$
# Two Sample Intervals
## Difference of Means: Known Variances
Let $\boldsymbol{X}=\left(X_1, \ldots, X_m\right)$ and $\boldsymbol{Y}=\left(Y_1, \ldots, Y_n\right)$ be two independent samples with normal distributions $N\left(\mu_x, \sigma_x^2\right)$ and $N\left(\mu_y, \sigma_y^2\right)$, respectively. Assume that the variances $\sigma_x^2$ and $\sigma_y^2$ are known.

To construct the confidence interval for the difference $\mu_x-\mu_y$, we use the sample means $\bar{X} \sim \mathrm{~N}\left(\mu_x, \sigma_x^2 / m\right)$ and $\bar{Y} \sim \mathrm{~N}\left(\mu_y, \sigma_y^2 / n\right)$ and recall that $\bar{X}-\bar{Y} \sim \mathrm{~N}\left(\mu_x-\mu_y, \sigma_x^2 / m+\sigma_y^2 / n\right)$, i.e.
$$
g\left(\mu_x-\mu_y ; \boldsymbol{X}, \boldsymbol{Y}\right)=\frac{\bar{X}-\bar{Y}-\left(\mu_x-\mu_y\right)}{\sqrt{\sigma_x^2 / m+\sigma_y^2 / n}} \sim \mathrm{~N}(0,1) .
$$
Hence, we can use $g\left(\mu_x-\mu_y ; \boldsymbol{X}, \boldsymbol{Y}\right)$ as a pivotal quantity for the parameter $\mu_x-\mu_y$, leading to the ( $1-\alpha$ )-confidence bounds:
$$
\bar{X}-\bar{Y}-z(\alpha / 2) \sqrt{\frac{\sigma_x^2}{m}+\frac{\sigma_y^2}{n}} \leq \mu_x-\mu_y \leq \bar{X}-\bar{Y}+z(\alpha / 2) \sqrt{\frac{\sigma_x^2}{m}+\frac{\sigma_y^2}{n}}
$$
Accordingly, the one-sided $(1-\alpha)$-confidence bounds are given by:
$$
\mu_x-\mu_y \geq \bar{X}-\bar{Y}-z(\alpha) \sqrt{\frac{\sigma_x^2}{m}+\frac{\sigma_y^2}{n}}
$$
$$
\mu_x-\mu_y \leq \bar{X}-\bar{Y}+z(\alpha) \sqrt{\frac{\sigma_x^2}{m}+\frac{\sigma_y^2}{n}}
$$
## Difference of Means: Unknown Equal Variances
Suppose now that the variances $\sigma_x^2$ and $\sigma_y^2$ are unknown. Under an additional assumption that they are equal, $\sigma_x^2=\sigma_y^2=\sigma^2$, we use the fact that
$$
g\left(\mu_x-\mu_y ; \boldsymbol{X}, \boldsymbol{Y}\right)=\frac{\bar{X}-\bar{Y}-\left(\mu_x-\mu_y\right)}{S_p \sqrt{1 / m+1 / n}} \sim t_{m+n-2}
$$
($t$-distribution with $m+n-2$ degrees of freedom), where
$$
S_p^2=\frac{(m-1) S_x^2+(n-1) S_y^2}{m+n-2}
$$
is the pulled estimate of the common variance $\sigma^2$, so that
$$
\frac{(m+n-2) S_p^2}{\sigma^2} \sim \chi_{m+n-2}^2 .
$$
Hence, the confidence bounds for $\mu_x-\mu_y$ are given by:
$$
\bar{X}-\bar{Y}-t_{m+n-2}(\alpha / 2) S_p \sqrt{\frac{1}{m}+\frac{1}{n}} \leq \mu_x-\mu_y \leq \bar{X}-\bar{Y}+t_{m+n-2}(\alpha / 2) S_p \sqrt{\frac{1}{m}+\frac{1}{n}}
$$
Accordingly, the one-sided ( $1-\alpha$ )-confidence bounds are given by:
$$
\begin{aligned}
& \mu_x-\mu_y \geq \bar{X}-\bar{Y}-t_{m+n-2}(\alpha) S_p \sqrt{\frac{1}{m}+\frac{1}{n}} \\
& \mu_x-\mu_y \leq \bar{X}-\bar{Y}+t_{m+n-2}(\alpha) S_p \sqrt{\frac{1}{m}+\frac{1}{n}}
\end{aligned}
$$
# Asymptotic Intervals
Often a pivotal quantity constructed on the basis of a random sample may not always be available, or perhaps its distribution not explicitly known. In such cases, we opt for constructing an asymptotic confidence interval, where the confidence probability is only achieved in the large sample limit. 
## General Method
The usual method is based on the asymptotic normality of point estimators, which may be due to Central Limit Theorem or more generally by virtue of asymptotic normality of maximum likelihood estimators.
Specifically, suppose an estimator $\hat{\theta}=\hat{\theta}(\boldsymbol{X})$ is consistent and asymptotically normal,
$$
\hat{\theta} \stackrel{d}{\approx} \mathrm{~N}\left(\theta, \frac{\sigma_*^2(\theta)}{n}\right),
$$
with some asymptotic variance $\sigma_*^2(\theta)$ being a known (continuous) function of the parameter $\theta$. As explained earlier, the notation $\stackrel{d}{\approx}$ indicates an approximation "in distribution". More precisely, as $n \rightarrow \infty$,
$$
\sqrt{n}(\hat{\theta}-\theta) \xrightarrow{d} \mathrm{~N}\left(0, \sigma_*^2(\theta)\right) .
$$
Hence, as a candidate for the asymptotic pivotal quantity, it is natural to consider
$$
g(\theta ; \boldsymbol{X})=\frac{\sqrt{n}(\hat{\theta}-\theta)}{\sigma_*(\theta)} \stackrel{d}{\approx} \mathrm{~N}(0,1)
$$
Indeed, the distribution of $g(\theta ; \boldsymbol{X})$ is asymptotically free of parameter $\theta$, and it only remains to check out if monotonicity in variable $\theta$ is in place (which is needed to solve the confidence inequalities in terms of $\theta$).

The latter may be difficult to verify (or even untrue), but we can replace $\theta$ with its estimate $\hat{\theta}$ thanks to consistency of the estimator, $\hat{\theta} \xrightarrow{p} \theta$, where, as before, the notation $\xrightarrow{p}$ indicates convergence "in probability". This will retain the asymptotic normality, leading to a simplified choice of the pivotal quantity:
$$
\hat{g}(\theta ; \boldsymbol{X})=\frac{\sqrt{n}(\hat{\theta}-\theta)}{\sigma_*(\hat{\theta})} \stackrel{d}{\approx} \mathrm{~N}(0,1)
$$
for which monotonicity in $\theta$ is now obvious.
To construct confidence bounds with an approximate confidence probability $1-\alpha$, we follow the usual procedure:
$$
\mathrm{P}(|\hat{g}(\theta ; \boldsymbol{X})| \leq z(\alpha / 2))=\mathrm{P}\left(\frac{\sqrt{n}|\hat{\theta}-\theta|}{\sigma_*(\hat{\theta})} \leq z(\alpha / 2)\right) \approx 1-\alpha .
$$
Expressing this as the two-sided bounds on $\theta$, we conclude that, with probability close to $1-\alpha$, we have
$$
\hat{\theta}-\frac{z(\alpha / 2) \sigma_*(\hat{\theta})}{\sqrt{n}} \leq \theta \leq \hat{\theta}+\frac{z(\alpha / 2) \sigma_*(\hat{\theta})}{\sqrt{n}}
$$
The length of this asymptotic confidence interval is given by $2 z(\alpha / 2) \sigma_*(\hat{\theta}) / \sqrt{n}$, which makes it clear that using more efficient estimators (i.e. those with a smaller variance $\sigma_*(\theta)$ ) would lead to shorter confidence intervals.

One-sided confidence bounds are obtained similarly:
$$
\theta \geq \hat{\theta}-\frac{z(\alpha) \sigma_*(\hat{\theta})}{\sqrt{n}}
$$
$$
\theta \leq \hat{\theta}+\frac{z(\alpha) \sigma_*(\hat{\theta})}{\sqrt{n}}
$$
## Maximum Likelihood Estimators
We know that maximum likelihood estimators (MLE) are asymptotically normal, $\hat{\theta} \stackrel{d}{\approx} \mathrm{~N}\left(\theta, 1 / I_n(\theta)\right)$, where $I_n(\theta)=n I(\theta)$ is Fisher's information. Equivalently,

$$
g(\theta ; \boldsymbol{X})=\sqrt{n I(\theta)(\hat{\theta}-\theta)} \stackrel{d}{\approx} \mathrm{~N}(0,1),
$$

and, as before, a simplified version is obtained by replacing $I(\theta)$ with $I(\hat{\theta})$ (as usual, assuming that $I(\theta)$ is continuous):
$$
\hat{g}(\theta ; \boldsymbol{X})=\sqrt{n I(\hat{\theta})(\hat{\theta}-\theta)} \stackrel{d}{\approx} \mathrm{~N}(0,1) .
$$
This can be used as a pivotal quantity, leading to the asymptotic confidence intervals:
$$
\hat{\theta}-\frac{z(\alpha / 2)}{\sqrt{n I(\hat{\theta})}} \leq \theta \leq \hat{\theta}+\frac{z(\alpha / 2)}{\sqrt{n I(\hat{\theta})}}
$$
## Example: Binomial Model
Let $X_i \sim \operatorname{Ber}(p)(0<p<1)$. By the Central Limit Theorem, the statistic $T_n=X_1+\cdots+X_n \sim \operatorname{Bin}(n, p)$ is asymptotically normal with mean $\mathrm{E}_p\left(T_n\right)=n p$ and variance $\operatorname{Var}_p\left(T_n\right)=n p(1-p)$ :
$$
\frac{T_n-n p}{\sqrt{n p(1-p)}}=\frac{\sqrt{n}(\bar{X}-p)}{\sqrt{p(1-p)}} \stackrel{d}{\approx} \mathrm{~N}(0,1),
$$
where $\bar{X}=T_n / n$ is the sample mean. Recalling that $\bar{X}$ is the maximum likelihood estimator of $p$, the same approximation follows from the general result about asymptotic normality of MLEs:
$$
\sqrt{n I(p)}(\hat{p}-p) \stackrel{d}{\approx} \mathrm{~N}(0,1),
$$
noting that the information is given by $I(p)=p^{-1}(1-p)^{-1}$.
Replacing $p$ in the denominator with the estimate $\hat{p}=\bar{x}$, a simplified normal approximation is
$$
\frac{\sqrt{n}(\bar{X}-p)}{\sqrt{\hat{p}(1-\hat{p})}} \stackrel{d}{\approx} \mathrm{~N}(0,1) .
$$
Hence, with confidence probability close to $1-\alpha$, we have
$$
\frac{\sqrt{n}|\bar{X}-p|}{\sqrt{\bar{X}(1-\bar{X})}} \leq z(\alpha / 2),
$$
where $z(\alpha / 2)$ is the upper ( $\alpha / 2$ )-quantile of the standard normal distribution $\mathrm{N}(0,1)$, i.e. $1-\Phi(z(\alpha / 2))=\alpha / 2$.

Solving for $p$, we obtain the asymptotic confidence interval
$$
p_1 \leq p \leq p_2,
$$
where the confidence bounds $p_{1,2}=p_{1,2}(\boldsymbol{X})$ are given by:
$$
p_{1,2}=\bar{X} \pm \frac{z(\alpha / 2) \sqrt{\bar{X}(1-\bar{X})}}{\sqrt{n}}
$$
