- Total Probability Rule 
$$P(A)=\sum^{n}_{i=1}P(A|B_{i})\cdot P(B_{i})$$
- Conditional Probability
$$P(A|B)=\frac{P(A\cup B)}{P(B)}$$
- Bayes' Theorem
$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$
- Expectation Value
$$E[X]=\sum^{}_{x}xp(x)\text{ or }\int_{x}xp(x)dx$$
- Variance
$$Var(X)=E[(X-\mu)^2] = E[X^2]-\mu^2$$
- Covariance
$$Cov(X,Y)=E[(X-\mu_x)(Y-\mu_y)]=E[XY]-E[X]E[Y]$$
$$Cov(X_1+X_2,Y)=Cov(X_1,Y)+Cov(X_2,Y)$$
- Sum of Variances by using the covariance definition
$$Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)$$
- Cauchy-Schwarz Inequality by considering $E[(tX+Y)^2]$ and noting that its always positive
$$E[XY]^2\le E[X^2]E[Y^2]$$
- Markov's Inequality, for $X\ge 0$, then for any $a>0$
$$E[X]\ge aP(X>a)$$
- Chebyshev's Inequality, for $X$ a random variable with mean $\mu$ and variance $\sigma^2$, then for any $k>0$, thus the probability that a random variable differs from the mean by more than $k$ standard deviations is bounded as $\frac{1}{k^2}$. 
$$P(|X-\mu|>k)\le \frac{\sigma^2}{k^2}\text{, or } P(|X-\mu|>k\sigma)\le \frac{1}{k^2}$$
- Weak Law of Large Numbers, where $S_n=X_1 + X_2 +...+X_n$
$$P\left(\left| \frac{S_n}{n}-\mu\right|>\varepsilon \right)\rightarrow0 \text{ as }n\rightarrow \infty$$
- Strong Law of Large Numbers
$$P\left(\frac{S_n}{n}\rightarrow \mu \text{ as }n\rightarrow \infty \right)=1$$
- Central Limit Theorem
$$P\left(\frac{S_n-n\mu}{\sqrt{n\sigma^2}}\leq x \right)\rightarrow\Phi(x)=\frac{1}{\sqrt{2\pi}}\int^x_{-\infty}e^{-u^2/2}du$$
	That is to say that the cumulative z-score distribution converges to a normal distribution $N(0,1)$. Written in another way $S_n\approx N(n\mu,n\sigma^2)$.


